{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import marshal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docker_agent_logger.app.src.AI import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = ( #.filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    tf.data.TextLineDataset(\"persistent_volume/data/HDFS_v1/HDFS.log\")\n",
    "    # .batch(128)\n",
    "    # .shuffle(buffer_size=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw_ds.take(100):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in raw_ds.batch(128):\n",
    "    count += 128\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max len, mean len and std len excluding outliers (len > 1000)\n",
    "seq_lens = []\n",
    "for i in raw_ds:\n",
    "    seq_lens.append(len(i.numpy()))\n",
    "    if len(i.numpy()) > 1000:\n",
    "        print(len(i.numpy()))\n",
    "\n",
    "seq_lens = np.array(seq_lens)\n",
    "print(seq_lens.max())\n",
    "print(seq_lens.mean())\n",
    "print(seq_lens.std())\n",
    "print(seq_lens.min())\n",
    "print(len(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4000\n",
    "max_len=60\n",
    "epochs=32\n",
    "chkpt = \"docker_agent_logger/app/classifier_labeled/\"\n",
    "MIN_TRAINING_SEQ_LEN = 1000\n",
    "\n",
    "raw_ds = (\n",
    "    tf.data.TextLineDataset(\"persistent_volume/data/HDFS_v1/HDFS.log\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    # .batch(128)\n",
    "    # .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "#             raw_ds,\n",
    "#             vocabulary_size=vocab_size,\n",
    "#             reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\",\"[EOS]\"],\n",
    "#         )\n",
    "\n",
    "# with open(\"docker_agent_logger/app/logs_tokenizer/vocab.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(vocab,f)\n",
    "\n",
    "with open(\"docker_agent_logger/app/logs_tokenizer/vocab.pkl\",\"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab,max_len=max_len)\n",
    "\n",
    "\n",
    "ds = raw_ds.map(tokenizer.preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "val_split = 0.2\n",
    "ds_size = 2614800\n",
    "\n",
    "train_size = int((1-val_split) * ds_size)\n",
    "val_size = int(val_split * ds_size)\n",
    "\n",
    "train_ds = ds.take(train_size).shuffle(buffer_size=train_size).batch(128)\n",
    "val_ds = raw_ds.skip(train_size).take(val_size)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(vocab_size = vocab_size,latent_dim=max_len//2,embedding_dim=128,max_len = max_len)\n",
    "\n",
    "model.vae.load_model(chkpt=chkpt) #17 for the other model\n",
    "\n",
    "# model.train_model(ds,epochs=epochs,chkpt=chkpt)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docker_agent_reader.app.src.AI_rnd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 400\n",
    "anomaly_detector = AnomalyDetector(latent_space_dim=max_len//2,threshold=threshold)\n",
    "data = {\"logs\":[],\"parsed_logs\":[],\"vectorized_logs\":[],\"encoded_logs\":[]}\n",
    "times = {\"parsed_logs\":[],\"vectorized_logs\":[],\"encoded_logs\":[],\"anomaly\":[],\"anomaly_rnd\":[]}\n",
    "recostruction_loss = []\n",
    "recostruction_loss_rnd = []\n",
    "\n",
    "\n",
    "for logs in val_ds.take(10000).batch(1):\n",
    "    t = time.time()\n",
    "    parsed_logs = tokenizer.parsing(logs)   \n",
    "    t_parse = time.time()\n",
    "    times[\"parsed_logs\"].append(t_parse-t)\n",
    "    vectorized_logs = tokenizer.vectorization(parsed_logs)\n",
    "    t_vectorize = time.time()\n",
    "    times[\"vectorized_logs\"].append(t_vectorize-t_parse)\n",
    "    encoded_logs = model.vae.encode(vectorized_logs)\n",
    "    t_encode = time.time()\n",
    "    times[\"encoded_logs\"].append(t_encode-t_vectorize)\n",
    "    losses = model.vae.train_step(vectorized_logs,train=False)\n",
    "\n",
    "    \n",
    "    anomaly = False\n",
    "    if losses[\"reconstruction_loss\"].numpy() > threshold:\n",
    "        anomaly = True\n",
    "        print(f\"anomaly detected with a reconstruction loss of {losses['reconstruction_loss'].numpy()}\")\n",
    "\n",
    "    t_anomaly = time.time()\n",
    "    times[\"anomaly\"].append(t_anomaly-t_encode)\n",
    "\n",
    "\n",
    "    recostruction_loss_rnd_value, anomaly_rnd = anomaly_detector.detect(encoded_logs)\n",
    "\n",
    "    times[\"anomaly_rnd\"].append(time.time()-t_anomaly)\n",
    "\n",
    "    anomaly_detector.train_step(encoded_logs)\n",
    "\n",
    "\n",
    "    recostruction_loss_rnd.append(recostruction_loss_rnd_value.numpy())\n",
    "    recostruction_loss.append(losses[\"reconstruction_loss\"].numpy())\n",
    "    compressed_data = bz2.compress(pickle.dumps(logs))\n",
    "    data[\"logs\"].append(sys.getsizeof(compressed_data))\n",
    "    compressed_data = bz2.compress(pickle.dumps(parsed_logs))\n",
    "    data[\"parsed_logs\"].append(sys.getsizeof(compressed_data))\n",
    "    compressed_data = bz2.compress(pickle.dumps(vectorized_logs))\n",
    "    data[\"vectorized_logs\"].append(sys.getsizeof(compressed_data))\n",
    "    compressed_data = bz2.compress(pickle.dumps(encoded_logs))\n",
    "    data[\"encoded_logs\"].append(sys.getsizeof(compressed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for key in data.keys():\n",
    "    ax.plot(data[key], label=key)\n",
    "    plt.xlabel('logs')\n",
    "    plt.ylabel('size [Bytes]')\n",
    "    plt.legend()\n",
    "\n",
    "fig.savefig(\"data/size.png\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for key in times.keys():\n",
    "    ax.plot(np.array(times[key][10:])*10**3, label=key)\n",
    "    plt.xlabel('logs')\n",
    "    plt.ylabel('time [ms]')\n",
    "    plt.legend()\n",
    "\n",
    "fig.savefig(\"data/time.png\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recostruction_loss)\n",
    "plt.xlabel('logs')\n",
    "plt.ylabel('reconstruction loss')\n",
    "\n",
    "fig.savefig(\"data/reconstruction_loss.png\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recostruction_loss_rnd)\n",
    "plt.xlabel('logs')\n",
    "plt.ylabel('reconstruction loss rnd')\n",
    "\n",
    "plt.savefig(\"data/reconstruction_loss_rnd.png\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = tf.random.normal(shape=(1,60 ,30),dtype=tf.float32)\n",
    "encode_token = val_ds.take(1).batch(1).as_numpy_iterator().next()\n",
    "\n",
    "print(encode_token)\n",
    "\n",
    "\n",
    "z = model.vae.encoder(tokenizer.preprocess(encode_token))[0]\n",
    "\n",
    "tokens = model.vae.decode(z)\n",
    "\n",
    "print((tokenizer.decode(tokens).numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_clusters(vae, name,data):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1])\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    # plt.savefig(\"./results/\"+name+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "# model = Model(vocab_size = vocab_size,latent_dim=256,embedding_dim=128,max_len = max_len)\n",
    "\n",
    "\n",
    "# for i in range(32):\n",
    "#     model.vae.load_model(chkpt=chkpt+str(i))\n",
    "\n",
    "#     plot_label_clusters(model.vae, \"cluster\"+str(i),ds)\n",
    "\n",
    "ds_val_pre = val_ds.map(tokenizer.preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "plot_label_clusters(model.vae, \"cluster\"+str(17),ds_val_pre.take(10000).batch(128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
